{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67203978-79a9-40d3-8217-3d001ebe34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LeakyReLU, Dropout, Input, BatchNormalization,Attention, Input, Multiply,Activation,Reshape\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from keras_adabound import AdaBound\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import sklearn.preprocessing\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "import warnings\n",
    "import keras\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_transform = Dense(input_shape[0][-1], activation='tanh')\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value = inputs\n",
    "        query_transformed = self.query_transform(query)\n",
    "        attention_scores = tf.keras.layers.Dot(axes=-1)([query_transformed, value])\n",
    "        attention_scores = tf.keras.layers.Softmax()(attention_scores)\n",
    "        context_vector = tf.keras.layers.Dot(axes=1)([attention_scores, value])\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, num_layers, d_model, num_heads, dff, training=False):\n",
    "    # Embedding layer\n",
    "    x = Dense(d_model, input_shape=inputs.shape[1:])(inputs)\n",
    "\n",
    "    # MultiHeadAttention layers\n",
    "    for _ in range(num_layers):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x, x, training=training)\n",
    "        x = tf.keras.layers.add([x, attention_output])  # residual connection\n",
    "        x = Dense(dff, activation='relu')(x)\n",
    "        x = Dense(d_model)(x)\n",
    "        x = Dropout(0.1)(x, training=training)\n",
    "\n",
    "    return x\n",
    "\n",
    "def MML(y_true, y_pred, Beta_level):\n",
    "    # print(y_true.shape)\n",
    "    # print(y_pred.shape)\n",
    "    # loss1 = tf.sqrt(tf.reduce_mean(tf.square(y_true[:, Beta_level] - y_pred[:, Beta_level]))) \n",
    "    loss1 = tf.reduce_mean(tf.square(y_true[:, Beta_level] - y_pred[:, Beta_level]))\n",
    "    excluded_dims = [Beta_level, 10]  \n",
    "\n",
    "    mask = K.ones_like(y_pred)\n",
    "    for dim in excluded_dims:\n",
    "        mask = K.concatenate([mask[:, :dim], K.zeros_like(mask[:, dim:dim+1]), mask[:, dim+1:]], axis=1)\n",
    "\n",
    "    masked_y_pred = y_pred * mask\n",
    "\n",
    "    Get_sum = K.sum(masked_y_pred, axis=-1, keepdims=True)\n",
    "    Beta=y_true[:, 10]\n",
    "    loss2=tf.reduce_mean(tf.square(Beta - Get_sum- y_pred[:, Beta_level]))\n",
    "    return loss1+loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b05c1c-8640-47d2-bb24-37456040cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMLSTT(data, Beta_level):\n",
    "    X = data[X1]\n",
    "    y = data[y1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    kernel_size = 3\n",
    "    dropout = 0.2\n",
    "    input_shape = X_train.shape[1:]\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = transformer_encoder(inputs=input_layer,num_layers=2, d_model=128, num_heads=4, dff=128)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Conv1D(32, kernel_size=3, activation=\"relu\", padding=\"causal\", dilation_rate=1)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"causal\", activation=\"relu\", dilation_rate=2)(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(128, kernel_size=3, padding=\"causal\", activation=\"relu\", dilation_rate=4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    output_layer = Dense()(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.compile(loss=lambda y_true, y_pred: GetMyloss(y_true, y_pred, Beta_level=Beta_level), optimizer= \"adam\")\n",
    "\n",
    "    model.fit(x=X_train, y=y_train, batch_size, epochs, validation_data=(X_test, y_test), verbose=1, callbacks=callbacks_list)\n",
    "    \n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
