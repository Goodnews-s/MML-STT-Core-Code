{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c46c35-e02b-4ae0-a446-883873183074",
   "metadata": {},
   "source": [
    "DATA PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad3f63-2558-4879-90d6-e6fcdc35735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import julian\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import xlwt\n",
    "\n",
    "def es_wexler(T, P):\n",
    "    if T >= 273.15:\n",
    "        \n",
    "        # Saturation Vapor Pressure over Water\n",
    "        g0 =-2.8365744*10**3\n",
    "        g1 =-6.028076559*10**3\n",
    "        g2 = 1.954263612*10**1\n",
    "        g3 =-2.737830188*10**-2\n",
    "        g4 = 1.6261698*10**-5\n",
    "        g5 = 7.0229056*10**-10\n",
    "        g6 =-1.8680009*10**-13\n",
    "        g7 = 2.7150305\n",
    "        es = 0.01 * np.exp(g0*T**-2 + g1*T**-1 + g2 + g3*T + g4*T**2 + g5*T**3 + g6*T**4 + g7*np.log(T))\n",
    "\n",
    "        # Enhancement Factor coefficients for Water 0 to 100°C\n",
    "        A0 =-1.6302041*10**-1\n",
    "        A1 = 1.8071570*10**-3\n",
    "        A2 =-6.7703064*10**-6\n",
    "        A3 = 8.5813609*10**-9\n",
    "        B0 =-5.9890467*10**1\n",
    "        B1 = 3.4378043*10**-1\n",
    "        B2 =-7.7326396*10**-4\n",
    "        B3 = 6.3405286*10**-7\n",
    "    else:\n",
    "        \n",
    "        # Saturation Vapor Pressure over Ice\n",
    "        k0 =-5.8666426*10**3\n",
    "        k1 = 2.232870244*10**1\n",
    "        k2 = 1.39387003*10**-2\n",
    "        k3 =-3.4262402*10**-5\n",
    "        k4 = 2.7040955*10**-8\n",
    "        k5 = 6.7063522*10**-1\n",
    "        es = 0.01 * np.exp(k0*T**-1 + k1 + k2*T + k3*T**2 + k4*T**3 + k5*np.log(T))\n",
    "\n",
    "        # Enhancement Factor coefficients for Ice –100 to 0°C\n",
    "        A0 =-6.0190570*10**-2\n",
    "        A1 = 7.3984060*10**-4\n",
    "        A2 =-3.0897838*10**-6\n",
    "        A3 = 4.3669918*10**-9\n",
    "        B0 =-9.4868712*10**1\n",
    "        B1 = 7.2392075*10**-1\n",
    "        B2 =-2.1963437*10**-3\n",
    "        B3 = 2.4668279*10**-6\n",
    "\n",
    "       # Enhancement Factor\n",
    "    alpha = A0 + A1*T + A2*T**2 + A3*T**3\n",
    "    beta = np.exp(B0 + B1*T + B2*T**2 + B3*T**3)\n",
    "    f = np.exp( alpha*(1-es/P) + beta*(P/es-1) )\n",
    "    return es * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80375b7-5976-4066-90c7-98431b6e84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_field_xls(path, sheet_name, value):\n",
    "    index = len(value)\n",
    "    workbook = xlwt.Workbook()\n",
    "    sheet = workbook.add_sheet(sheet_name)\n",
    "    for k in range(len(idx_R)):\n",
    "        sheet.write(0, k, idx_R[k])\n",
    "    for i in range(0, index):\n",
    "        for j in range(0, len(value[i])):\n",
    "            sheet.write(i+1, j, value[i][j])\n",
    "    workbook.save(path)\n",
    "\n",
    "def hgpt2(dt, x0, y0, z0, z0_type):\n",
    "\n",
    "    # Grid files location\n",
    "    coeffiles='' # put '/' or '\\' at the end\n",
    "\n",
    "    # Constants\n",
    "    row = 721\n",
    "    col = 1440\n",
    "    p1  = 365.250\n",
    "    p2  = 182.625\n",
    "    p3  = 91.3125\n",
    "\n",
    "    # Geographic coordinates ( equal to ERA5 )\n",
    "    lon = np.linspace(-180, 179.75, col)\n",
    "    lat = np.linspace(-90, 90, row)\n",
    "\n",
    "    # Modified Julian date\n",
    "    if np.size(dt) == 6:\n",
    "        # Input: Gregorian calendar\n",
    "        mjd = julian.to_jd(datetime(np.int(dt[0]),np.int(dt[1]),np.int(dt[2]),np.int(dt[3]),np.int(dt[4]),np.int(dt[5])), fmt='mjd')\n",
    "        hour = np.int(dt[3])\n",
    "    elif np.size(dt) == 1:\n",
    "        # Input: Modified Julian date\n",
    "        gre = julian.from_jd(dt, fmt='mjd')\n",
    "        mjd = dt\n",
    "        hour = np.int(np.around(gre.hour))\n",
    "    else:\n",
    "        raise NameError('Use 1) Modified Julian Date (MJD) or 2) Gregorian date (y,m,d,HH,MM,SS).')\n",
    "\n",
    "    # Finding indexes for bilinear interpolation\n",
    "    # x-location\n",
    "    indx = np.argsort( np.sqrt( (lon-x0)**2 ) )\n",
    "    ix1 = indx[ 0 ]; ix2 = indx[ 1 ]\n",
    "    x1 = lon[ ix1 ]; x2 = lon[ ix2 ]\n",
    "    x = [ix1, ix1, ix2, ix2]\n",
    "    # y-location\n",
    "    indy = np.argsort( np.sqrt( (lat-y0)**2 ) )\n",
    "    jy1 = indy[ 0 ]; jy2 = indy[ 1 ]\n",
    "    y1 = lat[ jy1 ]; y2 = lat[ jy2 ]\n",
    "    y = [jy1, jy2, jy1, jy2]\n",
    "    # xy-distances (weights)\n",
    "    dx1y1= 1/np.sqrt( (x1 - x0)**2 + (y1 - y0)**2 )\n",
    "    dx1y2= 1/np.sqrt( (x1 - x0)**2 + (y2 - y0)**2 )\n",
    "    dx2y1= 1/np.sqrt( (x2 - x0)**2 + (y1 - y0)**2 )\n",
    "    dx2y2= 1/np.sqrt( (x2 - x0)**2 + (y2 - y0)**2 )\n",
    "    dxy = np.array([dx1y1, dx1y2, dx2y1, dx2y2], dtype=np.float64)\n",
    "    if np.any(np.isinf(dxy)) == True:\n",
    "       # Exact point grid\n",
    "       dxy = np.array([1,0,0,0], dtype=np.float64)\n",
    "\n",
    "    # ******************************************************************\n",
    "    # Open and read the surface air temperature coefficients file\n",
    "    # ******************************************************************\n",
    "    fid = open(coeffiles+'temp_grid.bin', 'rb')\n",
    "    fid.seek((row*col*26)*hour, 0)\n",
    "    a0 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    b0 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    a1 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f1 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    a2 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f2 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    a3 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f3 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    fid.close()\n",
    "\n",
    "    # Surface air temperature model\n",
    "    fun_t = lambda a0, b0, a1, f1, a2, f2, a3, f3: a0 + b0*(mjd - 51178) + a1*np.cos(2*np.pi*(mjd - 51178)/p1+f1) + a2*np.cos(2*np.pi*(mjd - 51178)/p2+f2) + a3*np.cos(2*np.pi*(mjd - 51178)/p3+f3)\n",
    "\n",
    "    # Applying the bilinear interpolation\n",
    "    tij = np.array([0,0,0,0], dtype=np.float64)\n",
    "    for j in range(0, len(x)):\n",
    "        tij[j] = fun_t(a0[y[j],x[j]], b0[y[j],x[j]], a1[y[j],x[j]], f1[y[j],x[j]], a2[y[j],x[j]], f2[y[j],x[j]], a3[y[j],x[j]], f3[y[j],x[j]])\n",
    "    T = np.sum(tij*dxy)/np.sum(dxy)\n",
    "\n",
    "    # ******************************************************************\n",
    "    # Open and read the surface pressure coefficients file\n",
    "    # ******************************************************************\n",
    "    fid = open(coeffiles+'press_grid.bin', 'rb')\n",
    "    fid.seek((row*col*20)*hour, 0)\n",
    "    a0 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    b0 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    a1 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f1 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    a2 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f2 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    fid.close()\n",
    "\n",
    "    # Surface pressure model\n",
    "    fun_p = lambda a0, b0, a1, f1, a2, f2: a0 + b0*(mjd - 51178) + a1*np.cos(2*np.pi*(mjd - 51178)/p1+f1) + \\\n",
    "    a2*np.cos(2*np.pi*(mjd - 51178)/p2+f2)\n",
    "\n",
    "    # Applying the bilinear interpolation\n",
    "    pij = np.array([0,0,0,0], dtype=np.float64)\n",
    "    for j in range(0, len(x)):\n",
    "        pij[j] = fun_p(a0[y[j],x[j]], b0[y[j],x[j]], a1[y[j],x[j]], f1[y[j],x[j]], a2[y[j],x[j]], f2[y[j],x[j]])\n",
    "    P = np.sum(pij*dxy)/np.sum(dxy)\n",
    "\n",
    "    # ******************************************************************\n",
    "    # Open and read the surface relative humidity coefficients file\n",
    "    # ******************************************************************\n",
    "    fid = open(coeffiles+'rh_grid.bin', 'rb')\n",
    "    fid.seek((row*col*22)*hour, 0)\n",
    "    a0 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    a1 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f1 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    a2 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f2 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    a3 = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    f3 = np.fromfile(fid, dtype=np.int16, count=row*col).reshape((row, col), order='F')/10000.0\n",
    "    fid.close()\n",
    "\n",
    "    # Surface relative humidity model\n",
    "    fun_rh = lambda a0, a1, f1, a2, f2, a3, f3: a0 + a1*np.cos(2*np.pi*(mjd - 51178)/p1+f1) + \\\n",
    "    a2*np.cos(2*np.pi*(mjd - 51178)/p2+f2) + a3*np.cos(2*np.pi*(mjd - 51178)/p3+f3)\n",
    "\n",
    "    # Applying the bilinear interpolation\n",
    "    rhij = np.array([0,0,0,0], dtype=np.float64)\n",
    "    for j in range(0, len(x)):\n",
    "        rhij[j] = fun_rh(a0[y[j],x[j]], a1[y[j],x[j]], f1[y[j],x[j]],a2[y[j],x[j]], f2[y[j],x[j]], a3[y[j],x[j]], f3[y[j],x[j]])\n",
    "    RH = np.sum(rhij*dxy)/np.sum(dxy)\n",
    "\n",
    "    # ************************************************************\n",
    "    # Open and read the Tm coefficients and undulation file\n",
    "    # ************************************************************\n",
    "    fid = open(coeffiles+'tm_grid.bin', 'rb')\n",
    "    a = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    b = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    orography = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    undu = np.fromfile(fid, dtype=np.float32, count=row*col).reshape((row, col), order='F')\n",
    "    fid.close()\n",
    "\n",
    "    # Applying the bilinear interpolation\n",
    "    aij = np.array([0,0,0,0], dtype=np.float64)\n",
    "    bij = np.copy(aij)\n",
    "    hij = np.copy(aij)\n",
    "    nij = np.copy(aij)\n",
    "    for j in range(0, len(x)):\n",
    "        aij[j] = a[y[j],x[j]]\n",
    "        bij[j] = b[y[j],x[j]]\n",
    "        hij[j] = orography[y[j],x[j]]\n",
    "        nij[j] = undu[y[j],x[j]]\n",
    "    a = np.sum(aij*dxy)/np.sum(dxy)\n",
    "    b = np.sum(bij*dxy)/np.sum(dxy)\n",
    "    geo_height = np.sum(hij*dxy)/np.sum(dxy)\n",
    "    N = np.sum(nij*dxy)/np.sum(dxy)\n",
    "\n",
    "    # Zenith hydrostatic delay (ZHD), Saastamoinen model\n",
    "    if z0_type=='orth':\n",
    "        H_orth = z0\n",
    "    elif z0_type=='elli':\n",
    "        H_orth = z0 - N\n",
    "    else:\n",
    "        raise NameError('Use 1) <<orth>> for Orthometric height or 2) <<elli>> for Ellipsoidal height (in m).')\n",
    "\n",
    "    # Correction to P, T, and RH (see Guochanf Xu, GPS Theory, Algorithms and Applications, 3nd Edition, page 82)\n",
    "    dh = H_orth - geo_height\n",
    "\n",
    "    # Temperature lapse rate by latitude\n",
    "    # rate = (6.25 - 2*sin( np.deg2rad(y0) )^4)/1000\n",
    "    rate = (10.3 + 0.03182*(T-273.15) - 0.00436*P)/1000\n",
    "\n",
    "    # Altimetric corrections\n",
    "    P = (P*100 * ( 1 - (rate*dh)/T )**5.6004)/100\n",
    "    T = T - rate * ( dh )\n",
    "\n",
    "    # Ensure that we eliminate any noise outside this range\n",
    "    # Gamit has an unidentified error when assuming rh = 100% (met-files)\n",
    "    if RH >= 100:\n",
    "        RH = 99.9\n",
    "    if RH < 0:\n",
    "        RH = 0\n",
    "\n",
    "    # Call external function\n",
    "    es = es_wexler(T, P)\n",
    "    e = es * (RH/100)\n",
    "\n",
    "    # Weight mean temperature, Tm\n",
    "    Tm = a + b*T\n",
    "\n",
    "    # ZHD using the Saastamoinen Model (see Saastamoinen, 1973)\n",
    "    ZHD = (0.0022768 * P)/(1 - 0.0026*np.cos(2*np.deg2rad(y0))-0.00000028*H_orth)\n",
    "\n",
    "    # ZWD using the Saastamoinen Tropospheric Model (see Saastamoinen, 1972, 1973)\n",
    "    ZWD = 0.002277 * ((1255/T + 0.05) * e)\n",
    "\n",
    "    # PWV\n",
    "    # Dimensionless constant\n",
    "    const = 10**8 / (461525 * (22.9744 + 375463/Tm))\n",
    "    PWV = ZWD * const\n",
    "\n",
    "    return ZHD+ZWD\n",
    "\n",
    "\n",
    "data=pd.read_excel(data_path,header=None)\n",
    "write_field_xls(outpath,station_name,Last) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16325b6f-4901-49eb-a291-b2da38fbb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PyEMD import EEMD, EMD, Visualisation,CEEMDAN\n",
    "import pylab as plt\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for csv in filelist:\n",
    "    path=Path\n",
    "    df=pd.read_csv(path)\n",
    "    beita=(df['ztd']-df['HGPT2_ZTD']).values\n",
    "    test=[]\n",
    "    for k in range(len(df)):\n",
    "        test.append(k)\n",
    "    test=np.array(test)\n",
    "    IMF = CEEMDAN().ceemdan(beita,test,max_imf=10)\n",
    "    print(len(IMF))\n",
    "    new_df=df[df_index]\n",
    "    Beta=Beta_index\n",
    "    for k,imf in enumerate(IMF[0:10]):\n",
    "        new_df.insert(new_df.shape[1],Beta[k],imf)\n",
    "    new_df.insert(new_df.shape[1],'Beta',beita)\n",
    "    new_df.to_csv(outpath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61dbdce-591b-4009-a9f1-077ad5785a72",
   "metadata": {},
   "source": [
    "MODEL PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50225733-637c-4b79-8ac3-02d6b4f94351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LeakyReLU, Dropout, Input, BatchNormalization,Attention, Input, Multiply,Activation,Reshape\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from keras_adabound import AdaBound\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import sklearn.preprocessing\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "import warnings\n",
    "import keras\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_transform = Dense(input_shape[0][-1], activation='tanh')\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value = inputs\n",
    "        query_transformed = self.query_transform(query)\n",
    "        attention_scores = tf.keras.layers.Dot(axes=-1)([query_transformed, value])\n",
    "        attention_scores = tf.keras.layers.Softmax()(attention_scores)\n",
    "        context_vector = tf.keras.layers.Dot(axes=1)([attention_scores, value])\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, num_layers, d_model, num_heads, dff, training=False):\n",
    "    # Embedding layer\n",
    "    x = Dense(d_model, input_shape=inputs.shape[1:])(inputs)\n",
    "\n",
    "    # MultiHeadAttention layers\n",
    "    for _ in range(num_layers):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x, x, training=training)\n",
    "        x = tf.keras.layers.add([x, attention_output])  # residual connection\n",
    "        x = Dense(dff, activation='relu')(x)\n",
    "        x = Dense(d_model)(x)\n",
    "        x = Dropout(0.1)(x, training=training)\n",
    "\n",
    "    return x\n",
    "\n",
    "def MML(y_true, y_pred, Beta_level):\n",
    "    # print(y_true.shape)\n",
    "    # print(y_pred.shape)\n",
    "    # loss1 = tf.sqrt(tf.reduce_mean(tf.square(y_true[:, Beta_level] - y_pred[:, Beta_level]))) \n",
    "    loss1 = tf.reduce_mean(tf.square(y_true[:, Beta_level] - y_pred[:, Beta_level]))\n",
    "    excluded_dims = [Beta_level, 10]  \n",
    "\n",
    "    mask = K.ones_like(y_pred)\n",
    "    for dim in excluded_dims:\n",
    "        mask = K.concatenate([mask[:, :dim], K.zeros_like(mask[:, dim:dim+1]), mask[:, dim+1:]], axis=1)\n",
    "\n",
    "    masked_y_pred = y_pred * mask\n",
    "\n",
    "    Get_sum = K.sum(masked_y_pred, axis=-1, keepdims=True)\n",
    "    Beta=y_true[:, 10]\n",
    "    loss2=tf.reduce_mean(tf.square(Beta - Get_sum- y_pred[:, Beta_level]))\n",
    "    return loss1+loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1f21e-7eec-4ad3-a2f7-74391e0e194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMyloss(y_true, y_pred, Beta_level):\n",
    "    # print(y_true.shape)\n",
    "    # print(y_pred.shape)\n",
    "    # loss1 = tf.sqrt(tf.reduce_mean(tf.square(y_true[:, Beta_level] - y_pred[:, Beta_level]))) \n",
    "    loss1 = tf.reduce_mean(tf.square(y_true[:, Beta_level] - y_pred[:, Beta_level]))\n",
    "    excluded_dims = [Beta_level, 10]  \n",
    "\n",
    "\n",
    "    mask = K.ones_like(y_pred)\n",
    "    for dim in excluded_dims:\n",
    "        mask = K.concatenate([mask[:, :dim], K.zeros_like(mask[:, dim:dim+1]), mask[:, dim+1:]], axis=1)\n",
    " \n",
    "    masked_y_pred = y_pred * mask\n",
    "\n",
    "    Get_sum = K.sum(masked_y_pred, axis=-1, keepdims=True)\n",
    "    Beta=y_true[:, 10]\n",
    "    loss2=tf.reduce_mean(tf.square(Beta - Get_sum- y_pred[:, Beta_level]))\n",
    "    return loss1+loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40ed4b-84a2-4083-861c-3c7db8518273",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edadee-8444-4d0e-928c-61914ae8e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCN_Beta(data, Beta_level):\n",
    "    X = data[['lat', 'lon', 'hig', 'year', 'mouth', 'day', 'hour', 'P', 'T']]\n",
    "    y = data[['beita1', 'beita2', 'beita3', 'beita4', 'beita5', 'beita6', 'beita7', 'beita8', 'beita9', 'beita10', 'Beta']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    kernel_size = 3\n",
    "    dropout = 0.2\n",
    "    input_shape = X_train.shape[1:]\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = transformer_encoder(inputs=input_layer,num_layers=2, d_model=128, num_heads=4, dff=128)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    # TCN部分\n",
    "    x = Conv1D(32, kernel_size=3, activation=\"relu\", padding=\"causal\", dilation_rate=1)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"causal\", activation=\"relu\", dilation_rate=2)(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, padding=\"causal\", activation=\"relu\", dilation_rate=4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    \n",
    "    # 展平和全连接层\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dense(88, activation=\"relu\")(x)\n",
    "    x = Dense(44, activation=\"relu\")(x)\n",
    "    x = Dense(22, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    output_layer = Dense(11)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    filepath = str(Beta_level) + '\\\\weights.best.keras'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.compile(loss=lambda y_true, y_pred: GetMyloss(y_true, y_pred, Beta_level=Beta_level), optimizer= \"adam\")\n",
    "\n",
    "    model.fit(x=X_train, y=y_train, batch_size=500, epochs=300, validation_data=(X_test, y_test), verbose=1, callbacks=callbacks_list)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201cda3-a6b0-41d7-8cba-439c53cae72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('EXAMPLE_COLD.xlsx')\n",
    "#data=pd.read_excel('EXAMPLE_WARM.xlsx')#切换到自己的cold数据路径下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ccfee-d10e-431d-8401-61420cd7c539",
   "metadata": {},
   "source": [
    "Generate forecast information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec589b-6fb6-41ac-8eea-9fa394075901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beita_=[]\n",
    "for i in range(10):\n",
    "    X=data[['lat','lon','hig','year','mouth','day','hour','P','T']]\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    model_path=str(i+1)+'\\\\weights.best.keras'\n",
    "    model=load_model(model_path,custom_objects={'GetMyloss': GetMyloss,'MultiHeadAttention':MultiHeadAttention}, safe_mode=False)\n",
    "    pred=model.predict(X_scaled)\n",
    "    Beita_.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c97735-0dab-423d-8e3c-c2faa5e64b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beita_pred=[]\n",
    "for k in range(len(Beita_)):\n",
    "    for i in range(len(Beita_[0])):\n",
    "        beita_=[]\n",
    "        beita=Beita_[k][i][0]\\\n",
    "             +Beita_[k][i][1]\\\n",
    "             +Beita_[k][i][2]\\\n",
    "             +Beita_[k][i][3]\\\n",
    "             +Beita_[k][i][4]\\\n",
    "             +Beita_[k][i][5]\\\n",
    "             +Beita_[k][i][6]\\\n",
    "             +Beita_[k][i][7]\\\n",
    "             +Beita_[k][i][8]\\\n",
    "             +Beita_[k][i][9]\n",
    "        beita_.append(beita)\n",
    "        Beita_pred.append(beita_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1be41c-872f-4884-8795-4de4cec443cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztd_pred_1= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[0][i] for i in range(len(data))]\n",
    "ztd_pred_2= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[1][i] for i in range(len(data))]\n",
    "ztd_pred_3= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[2][i] for i in range(len(data))]\n",
    "ztd_pred_4= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[3][i] for i in range(len(data))]\n",
    "ztd_pred_5= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[4][i] for i in range(len(data))]\n",
    "ztd_pred_6= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[5][i] for i in range(len(data))]\n",
    "ztd_pred_7= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[6][i] for i in range(len(data))]\n",
    "ztd_pred_8= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[7][i] for i in range(len(data))]\n",
    "ztd_pred_9= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[8][i] for i in range(len(data))]\n",
    "ztd_pred_10= [data['HGPT2_ZTD'].values.tolist()[i] + Beita_pred[9][i] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a6482-888a-49c6-b0f1-172aeb088de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_1=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_1))\n",
    "rmse_2=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_2))\n",
    "rmse_3=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_3))\n",
    "rmse_4=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_4))\n",
    "rmse_5=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_5))\n",
    "rmse_6=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_6))\n",
    "rmse_7=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_7))\n",
    "rmse_8=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_8))\n",
    "rmse_9=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_9))\n",
    "rmse_10=math.sqrt(mean_squared_error(data['ztd'],ztd_pred_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36198a-cf53-45a0-a235-641e1595ff2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
